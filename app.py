import streamlit as st
import numpy as np
import pandas as pd
import re
from collections import Counter
import jieba

# --- é é¢è¨­å®š ---
st.set_page_config(
    page_title="AI/Human Detector Pro (Multi-lang)",
    page_icon="ğŸ‡¨ğŸ‡³",
    layout="wide"
)

# --- å´é‚Šæ¬„è¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š (Settings)")
    lang_mode = st.radio(
        "é¸æ“‡èªè¨€æ¨¡å¼ (Language Mode)",
        ["Traditional Chinese (ç¹ä¸­)", "English"]
    )
    
    st.info("â„¹ï¸ ä¸­æ–‡æ¨¡å¼ä½¿ç”¨ `jieba` é€²è¡Œæ–·è©æŠ€è¡“åˆ†æã€‚")

st.title(f"ğŸ“Š {lang_mode.split('(')[0]} æ–‡æœ¬ç‰¹å¾µåˆ†æå™¨")
st.markdown("æ­¤å·¥å…·é€éçµ±è¨ˆå­¸ç‰¹å¾µï¼ˆå¥é•·è®Šç•°æ•¸ã€è©å½™è±å¯Œåº¦ï¼‰è¼”åŠ©åˆ¤æ–·æ˜¯å¦ç‚º AI ç”Ÿæˆã€‚")

# --- åœç”¨è©è¨­å®š (éæ¿¾ç„¡æ„ç¾©è©å½™) ---
STOPWORDS_EN = set(['the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'of', 'in', 'on', 'at', 'to', 'it', 'this', 'that'])
STOPWORDS_ZH = set(['çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'èˆ‡', 'è‘—', 'æˆ–', 'ä¸€å€‹', 'æ²’æœ‰', 'æˆ‘å€‘', 'ä½ å€‘', 'ä»–å€‘', 'åœ¨', 'é€™', 'é‚£'])

# --- æ ¸å¿ƒé‚è¼¯ ---
def analyze_text_features(text, mode):
    clean_text = text.strip()
    if not clean_text:
        return None

    sentences = []
    words = []
    filtered_words = []

    # === é‡å°ä¸åŒèªè¨€çš„è™•ç†é‚è¼¯ ===
    if mode == "English":
        # è‹±æ–‡ï¼šç”¨ . ! ? åˆ‡å¥ï¼Œç”¨ç©ºç™½åˆ‡è©
        sentences = re.split(r'[.!?\n]+', clean_text)
        words = re.findall(r'\w+', clean_text.lower())
        stopwords = STOPWORDS_EN
        
    else: # Traditional Chinese
        # ä¸­æ–‡ï¼šç”¨ ã€‚ ï¼ ï¼Ÿ \n åˆ‡å¥
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', clean_text)
        # ä½¿ç”¨ jieba æ–·è©
        words = list(jieba.cut(clean_text))
        # éæ¿¾æ‰æ¨™é»ç¬¦è™Ÿèˆ‡ç©ºç™½
        words = [w for w in words if w.strip() and len(w) > 0]
        stopwords = STOPWORDS_ZH

    # ç§»é™¤ç©ºå¥å­
    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]

    if len(words) < 5:
        return None

    # --- ç‰¹å¾µè¨ˆç®— (ä¸­è‹±é€šç”¨) ---
    
    # å¥é•·è¨ˆç®— (ä¸­æ–‡ç®—è©æ•¸ï¼Œä¹Ÿå¯ä»¥æ”¹ç®—å­—æ•¸ï¼Œé€™è£¡çµ±ä¸€ç®—è©æ•¸/Tokenæ•¸)
    if mode == "English":
        sentence_lengths = [len(s.split()) for s in sentences]
    else:
        # ä¸­æ–‡å¥é•·ï¼šè¨ˆç®—è©²å¥åˆ‡åˆ†å¾Œçš„è©æ•¸
        sentence_lengths = [len(list(jieba.cut(s))) for s in sentences]
    
    avg_len = np.mean(sentence_lengths)
    std_dev = np.std(sentence_lengths) if len(sentence_lengths) > 1 else 0

    # è©å½™è±å¯Œåº¦
    unique_words = set(words)
    ttr = len(unique_words) / len(words)

    # éæ¿¾åœç”¨è© (ç‚ºäº†ç•«åœ–å¥½çœ‹)
    filtered_words = [w for w in words if w not in stopwords and len(w) > 1] # ä¸­æ–‡é€šå¸¸éæ¿¾å–®å­—è©
    word_counts = Counter(filtered_words)

    # --- è©•åˆ†é‚è¼¯ (Heuristic) ---
    score = 0.5 
    
    # èª¿æ•´é–¾å€¼ï¼šä¸­æ–‡çš„æ–·å¥ç¿’æ…£è·Ÿè‹±æ–‡ç•¥æœ‰ä¸åŒï¼Œç¨å¾®å¯¬é¬†ä¸€é»
    if std_dev < 4: score += 0.25      # æ¥µåº¦å¹³ç©© -> AI
    elif std_dev > 10: score -= 0.25   # æ³¢å‹•å¤§ -> Human

    if ttr < 0.4: score += 0.15        # ç”¨è©é‡è¤‡ -> AI
    elif ttr > 0.65: score -= 0.15     # ç”¨è©è±å¯Œ -> Human

    final_score = min(max(score, 0.01), 0.99)
    
    return {
        "score": final_score,
        "sentences": sentences,
        "sentence_lengths": sentence_lengths,
        "avg_len": avg_len,
        "std_dev": std_dev,
        "ttr": ttr,
        "word_counts": word_counts,
        "total_words": len(words),
        "total_sentences": len(sentences)
    }

# --- UI ä»‹é¢ ---
col_input, col_result = st.columns([1, 2])

with col_input:
    st.subheader("ğŸ“ è¼¸å…¥å€")
    placeholder_text = "è«‹è²¼ä¸Šä¸­æ–‡æ–‡ç« ..." if "Chinese" in lang_mode else "Paste English text here..."
    user_input = st.text_area("Input Text", height=300, placeholder=placeholder_text, label_visibility="collapsed")
    analyze_btn = st.button("ğŸš€ é–‹å§‹æ·±åº¦åˆ†æ", type="primary")

if analyze_btn and user_input:
    # å‘¼å«åˆ†æå‡½æ•¸ï¼Œå‚³å…¥èªè¨€æ¨¡å¼
    data = analyze_text_features(user_input, lang_mode)
    
    if data is None:
        st.warning("âš ï¸ æ–‡æœ¬éçŸ­ï¼Œç„¡æ³•åˆ†æã€‚")
    else:
        with col_result:
            st.subheader("ğŸ” åˆ†æå ±å‘Š")
            
            ai_score = data['score']
            if ai_score > 0.6:
                result_text = "é«˜åº¦ç–‘ä¼¼ AI ç”Ÿæˆ"
                result_color = "red"
            elif ai_score < 0.4:
                result_text = "å¯èƒ½æ˜¯ Human æ’°å¯«"
                result_color = "green"
            else:
                result_text = "æ··åˆç‰¹å¾µ / ä¸ç¢ºå®š"
                result_color = "orange"

            st.markdown(f"""
            <div style="padding:15px; border-radius:10px; background-color:rgba(128,128,128,0.1); border-left: 5px solid {result_color}">
                <h3 style="margin:0; color:{result_color}">{result_text} (AI æŒ‡æ•¸: {int(ai_score*100)}%)</h3>
            </div>
            """, unsafe_allow_html=True)
            
            st.write("")

            kpi1, kpi2, kpi3, kpi4 = st.columns(4)
            kpi1.metric("ç¸½å¥å­æ•¸", data['total_sentences'])
            kpi2.metric("å¹³å‡å¥é•· (è©)", f"{data['avg_len']:.1f}")
            kpi3.metric("å¥é•·æ³¢å‹• (Std Dev)", f"{data['std_dev']:.1f}")
            kpi4.metric("è©å½™è±å¯Œåº¦ (TTR)", f"{data['ttr']:.2f}")

            tab1, tab2 = st.tabs(["ğŸ“ˆ å¥å‹çµæ§‹åˆ†æ", "ğŸ”  å¸¸ç”¨è©å½™çµ±è¨ˆ"])

            with tab1:
                st.caption("è§€å¯Ÿé‡é»ï¼šäººé¡å¯«ä½œæ™‚ï¼Œå¥å­é•·åº¦ï¼ˆè©æ•¸ï¼‰é€šå¸¸æœƒæœ‰åŠ‡çƒˆæ³¢å‹•ã€‚")
                chart_data = pd.DataFrame({
                    "å¥å­é †åº": range(1, len(data['sentence_lengths']) + 1),
                    "è©æ•¸": data['sentence_lengths']
                })
                st.line_chart(chart_data, x="å¥å­é †åº", y="è©æ•¸", color="#FF4B4B")

            with tab2:
                st.caption("æ’é™¤å¸¸è¦‹åŠ©è©ï¼ˆçš„ã€äº†ã€æ˜¯...ï¼‰å¾Œçš„é—œéµå­—ã€‚")
                top_words = data['word_counts'].most_common(10)
                if top_words:
                    words_df = pd.DataFrame(top_words, columns=["è©å½™", "æ¬¡æ•¸"])
                    st.bar_chart(words_df.set_index("è©å½™"))
                else:
                    st.info("é—œéµå­—æ•¸æ“šä¸è¶³")

elif not analyze_btn:
    with col_result:
        st.info("ğŸ‘ˆ è«‹é¸æ“‡èªè¨€æ¨¡å¼ï¼Œè¼¸å…¥æ–‡ç« ä¸¦åˆ†æ")