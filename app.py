import streamlit as st
import numpy as np
import pandas as pd
import re
from collections import Counter

# --- é é¢è¨­å®š ---
st.set_page_config(
    page_title="AI/Human Detector Pro",
    page_icon="ğŸ“Š",
    layout="wide" # æ”¹ç‚ºå¯¬è¢å¹•æ¨¡å¼ä»¥å®¹ç´åœ–è¡¨
)

st.title("ğŸ“Š AI vs Human æ–‡æœ¬ç‰¹å¾µåˆ†æå™¨")
st.markdown("æ­¤å·¥å…·é€éçµ±è¨ˆå­¸ç‰¹å¾µï¼ˆå¥é•·è®Šç•°æ•¸ã€è©å½™è±å¯Œåº¦ï¼‰å°‡æ–‡æœ¬ã€Œè¦–è¦ºåŒ–ã€ï¼Œä»¥è¼”åŠ©åˆ¤æ–·æ˜¯å¦ç‚º AI ç”Ÿæˆã€‚")

# --- ç°¡å–®çš„åœç”¨è©è¡¨ (ç‚ºäº†éæ¿¾æ‰ the, a, is é€™ç¨®ç„¡æ„ç¾©è©) ---
STOPWORDS = set([
    'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 
    'of', 'in', 'on', 'at', 'to', 'for', 'with', 'by', 'it', 'this', 'that'
])

# --- æ ¸å¿ƒé‚è¼¯ï¼šç‰¹å¾µæå–èˆ‡åˆ†æ ---
def analyze_text_features(text):
    clean_text = text.strip()
    if not clean_text:
        return None

    # 1. åˆ‡åˆ†å¥å­
    sentences = re.split(r'[.!?]+', clean_text)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]
    
    # 2. åˆ‡åˆ†å–®å­—
    words = re.findall(r'\w+', clean_text.lower())
    
    if len(words) < 5:
        return None

    # --- ç‰¹å¾µè¨ˆç®— ---
    # å¥é•·åˆ—è¡¨
    sentence_lengths = [len(s.split()) for s in sentences]
    
    # å¹³å‡å¥é•·èˆ‡æ¨™æº–å·®
    avg_len = np.mean(sentence_lengths)
    std_dev = np.std(sentence_lengths) if len(sentence_lengths) > 1 else 0

    # è©å½™è±å¯Œåº¦ (Type-Token Ratio)
    unique_words = set(words)
    ttr = len(unique_words) / len(words)

    # éæ¿¾å¾Œçš„è©é » (ç§»é™¤åœç”¨è©)
    filtered_words = [w for w in words if w not in STOPWORDS]
    word_counts = Counter(filtered_words)

    # --- è©•åˆ†é‚è¼¯ ---
    score = 0.5 
    # AI å‚¾å‘æ–¼æ¨™æº–å·®å° (å¹³ç©©)
    if std_dev < 6: score += 0.25
    elif std_dev > 12: score -= 0.25 # Human å‚¾å‘æ–¼æ¨™æº–å·®å¤§ (æ³¢å‹•)

    # AI å‚¾å‘æ–¼è±å¯Œåº¦ä½ (é‡è¤‡)
    if ttr < 0.45: score += 0.15
    elif ttr > 0.65: score -= 0.15

    final_score = min(max(score, 0.01), 0.99)
    
    return {
        "score": final_score,
        "sentences": sentences,
        "sentence_lengths": sentence_lengths,
        "avg_len": avg_len,
        "std_dev": std_dev,
        "ttr": ttr,
        "word_counts": word_counts,
        "total_words": len(words),
        "total_sentences": len(sentences)
    }

# --- UI ä»‹é¢ ---
col_input, col_result = st.columns([1, 2]) # å·¦çª„å³å¯¬

with col_input:
    st.subheader("ğŸ“ è¼¸å…¥å€")
    user_input = st.text_area(
        "è«‹è²¼ä¸Šè‹±æ–‡æ–‡ç« ",
        height=300,
        placeholder="è²¼ä¸Šä½ çš„æ–‡ç« ..."
    )
    analyze_btn = st.button("ğŸš€ é–‹å§‹æ·±åº¦åˆ†æ", type="primary")

# --- åˆ†æçµæœé¡¯ç¤º ---
if analyze_btn and user_input:
    data = analyze_text_features(user_input)
    
    if data is None:
        st.warning("âš ï¸ æ–‡æœ¬éçŸ­ï¼Œç„¡æ³•é€²è¡Œæœ‰æ•ˆçµ±è¨ˆåˆ†æã€‚")
    else:
        with col_result:
            st.subheader("ğŸ” åˆ†æå ±å‘Š")
            
            # 1. é ‚éƒ¨çµæœå¡ç‰‡
            ai_score = data['score']
            if ai_score > 0.6:
                result_text = "é«˜åº¦ç–‘ä¼¼ AI ç”Ÿæˆ"
                result_color = "red"
            elif ai_score < 0.4:
                result_text = "å¯èƒ½æ˜¯ Human æ’°å¯«"
                result_color = "green"
            else:
                result_text = "æ··åˆç‰¹å¾µ / ä¸ç¢ºå®š"
                result_color = "orange"

            st.markdown(f"""
            <div style="padding:15px; border-radius:10px; background-color:rgba(128,128,128,0.1); border-left: 5px solid {result_color}">
                <h3 style="margin:0; color:{result_color}">{result_text} (AI æŒ‡æ•¸: {int(ai_score*100)}%)</h3>
            </div>
            """, unsafe_allow_html=True)
            
            st.write("") # Spacer

            # 2. é—œéµæŒ‡æ¨™ (KPIs)
            kpi1, kpi2, kpi3, kpi4 = st.columns(4)
            kpi1.metric("ç¸½å¥å­æ•¸", data['total_sentences'])
            kpi2.metric("å¹³å‡å¥é•· (å­—)", f"{data['avg_len']:.1f}")
            kpi3.metric("å¥é•·æ³¢å‹• (Std Dev)", f"{data['std_dev']:.1f}", help="æ•¸å€¼è¶Šé«˜ä»£è¡¨é•·çŸ­å¥äº¤éŒ¯è¶Šæ˜é¡¯ (Humanç‰¹å¾µ)")
            kpi4.metric("è©å½™è±å¯Œåº¦ (TTR)", f"{data['ttr']:.2f}", help="æ•¸å€¼è¶Šé«˜ä»£è¡¨ç”¨è©è¶Šä¸é‡è¤‡")

            # 3. åˆ†é é¡¯ç¤ºåœ–è¡¨
            tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ å¥å‹çµæ§‹åˆ†æ", "ğŸ”  å¸¸ç”¨è©å½™çµ±è¨ˆ", "ğŸ“„ åŸå§‹æ•¸æ“š"])

            with tab1:
                st.markdown("**å¥é•·æ³¢å‹•åœ– (Sentence Burstiness)**")
                st.caption("AI é€šå¸¸åƒæ©Ÿå™¨äººä¸€æ¨£è¦å¾‹ (ç·šæ¢å¹³ç·©)ï¼Œäººé¡å¯«ä½œå‰‡æƒ…ç·’èµ·ä¼å¤§ (ç·šæ¢åŠ‡çƒˆè·³å‹•)ã€‚")
                
                # å»ºç«‹ DataFrame çµ¦åœ–è¡¨ç”¨
                chart_data = pd.DataFrame({
                    "å¥å­é †åº": range(1, len(data['sentence_lengths']) + 1),
                    "å¥å­é•·åº¦ (å–®å­—æ•¸)": data['sentence_lengths']
                })
                
                st.line_chart(
                    chart_data, 
                    x="å¥å­é †åº", 
                    y="å¥å­é•·åº¦ (å–®å­—æ•¸)",
                    color="#FF4B4B"
                )

            with tab2:
                st.markdown("**é«˜é »è©å½™ (Top Keywords)**")
                st.caption("æ’é™¤å¸¸è¦‹ä»‹ç³»è©å¾Œçš„é—œéµå­—åˆ†ä½ˆã€‚")
                
                # å–å‡ºå‰ 10 å
                top_words = data['word_counts'].most_common(10)
                if top_words:
                    words_df = pd.DataFrame(top_words, columns=["å–®å­—", "å‡ºç¾æ¬¡æ•¸"])
                    st.bar_chart(words_df.set_index("å–®å­—"))
                else:
                    st.info("æ²’æœ‰è¶³å¤ çš„é—œéµå­—è³‡æ–™ã€‚")

            with tab3:
                st.json({
                    "AI_Score": data['score'],
                    "Sentence_Lengths": data['sentence_lengths'],
                    "Sentences": data['sentences']
                })

elif not analyze_btn:
    with col_result:
        st.info("ğŸ‘ˆ è«‹åœ¨å·¦å´è¼¸å…¥æ–‡ç« ä¸¦æŒ‰ä¸‹åˆ†ææŒ‰éˆ•")