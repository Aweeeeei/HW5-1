import streamlit as st
import numpy as np
import pandas as pd
import re
from collections import Counter
import jieba

# --- é é¢è¨­å®š ---
st.set_page_config(
    page_title="AI/Human Detector Pro",
    page_icon="ğŸ¤–",
    layout="wide"
)

# --- å®šç¾©ç¯„ä¾‹è³‡æ–™åº« (ä¸­è‹±æ–‡å„ 2 AI / 2 Human) ---
EXAMPLES = {
    "English": [
        {
            "type": "AI",
            "text": "Artificial Intelligence involves the development of algorithms that allow computers to perform tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, and language understanding. Machine learning, a subset of AI, focuses on building systems that learn from data. As technology advances, AI is becoming increasingly integrated into various sectors, including healthcare, finance, and transportation."
        },
        {
            "type": "Human",
            "text": "I literally cannot believe what just happened to me at the coffee shop! So, I ordered my usual latte, right? And the barista, who looked totally asleep, handed me a cup that felt way too light. I took a sip andâ€”BAMâ€”it was just hot milk! No coffee at all. Seriously? I just stood there laughing because, honestly, it's been that kind of week. Who forgets the coffee in a latte?"
        },
        {
            "type": "AI",
            "text": "Climate change refers to significant changes in global temperature and weather patterns over time. While climate change is a natural phenomenon, scientific evidence suggests that human activities, particularly the burning of fossil fuels, are the primary drivers of recent warming trends. This leads to rising sea levels, more frequent extreme weather events, and disruptions to ecosystems."
        },
        {
            "type": "Human",
            "text": "You know that feeling when you finish a really good book and you just stare at the wall for like ten minutes? That was me last night. The ending was so unexpected, yet it made perfect sense. I was crying, smiling, just a total mess. I wish I could erase my memory and read it all over again for the first time. Truly a masterpiece."
        }
    ],
    "Traditional Chinese (ç¹ä¸­)": [
        {
            "type": "AI",
            "text": "å€å¡ŠéˆæŠ€è¡“æ˜¯ä¸€ç¨®å»ä¸­å¿ƒåŒ–çš„åˆ†æ•£å¼å¸³æœ¬æŠ€è¡“ï¼Œå®ƒç¢ºä¿äº†è³‡æ–™çš„é€æ˜æ€§èˆ‡ä¸å¯ç¯¡æ”¹æ€§ã€‚æ¯ä¸€å€‹å€å¡Šéƒ½åŒ…å«äº†å‰ä¸€å€‹å€å¡Šçš„åŠ å¯†é›œæ¹Šå€¼ã€æ™‚é–“æˆ³è¨˜ä»¥åŠäº¤æ˜“è³‡æ–™ã€‚é€™ç¨®çµæ§‹ä½¿å¾—å€å¡Šéˆåœ¨é‡‘èç§‘æŠ€ã€ä¾›æ‡‰éˆç®¡ç†ä»¥åŠæ™ºæ…§åˆç´„ç­‰é ˜åŸŸå±•ç¾å‡ºå·¨å¤§çš„æ‡‰ç”¨æ½›åŠ›ã€‚éš¨è‘—æŠ€è¡“çš„æˆç†Ÿï¼Œæˆ‘å€‘é è¨ˆå°‡çœ‹åˆ°æ›´å¤šåŸºæ–¼å€å¡Šéˆçš„å‰µæ–°è§£æ±ºæ–¹æ¡ˆã€‚"
        },
        {
            "type": "Human",
            "text": "æ˜¨å¤©è·Ÿæœ‹å‹å»æ’é‚£å®¶æ–°é–‹çš„æ‹‰éºµåº—ï¼ŒçœŸçš„æ’åˆ°å¤©è’åœ°è€ï¼æˆ‘å€‘åœ¨å¯’é¢¨ä¸­ç«™äº†å¿«å…©å€‹å°æ™‚ï¼Œè…³éƒ½å¿«æ–·äº†ã€‚çµæœé€²å»ä¸€åƒï¼Œå“‡è³½ï¼Œé‚£å€‹æ¹¯é ­æ¿ƒéƒåˆ°ä¸è¡Œï¼Œå‰ç‡’ä¹Ÿæ˜¯å…¥å£å³åŒ–ï¼Œç¬é–“è¦ºå¾—å‰›å‰›çš„è¾›è‹¦éƒ½å€¼å¾—äº†ã€‚é›–ç„¶é€™å®¶åº—çš„åƒ¹æ ¼æœ‰é»å°è²´ï¼Œä½†ä¹…ä¹…åƒä¸€æ¬¡çŠ’è³è‡ªå·±æ‡‰è©²ä¸éåˆ†å§ï¼Ÿä¸‹æ¬¡ä¸€å®šè¦æŒ‘å¹³æ—¥ä¾†ï¼Œä¸ç„¶çœŸçš„æœƒç­‰åˆ°ç˜‹æ‰ã€‚"
        },
        {
            "type": "AI",
            "text": "å…‰åˆä½œç”¨æ˜¯æ¤ç‰©ã€è—»é¡å’ŒæŸäº›ç´°èŒåˆ©ç”¨é™½å…‰å°‡äºŒæ°§åŒ–ç¢³å’Œæ°´è½‰åŒ–ç‚ºè‘¡è„ç³–å’Œæ°§æ°£çš„éç¨‹ã€‚é€™å€‹éç¨‹å°æ–¼åœ°çƒä¸Šçš„ç”Ÿå‘½è‡³é—œé‡è¦ï¼Œå› ç‚ºå®ƒä¸åƒ…æä¾›äº†é£Ÿç‰©éˆçš„åŸºç¤èƒ½é‡ï¼Œé‚„é‡‹æ”¾äº†ç”Ÿç‰©å‘¼å¸æ‰€éœ€çš„æ°§æ°£ã€‚å…‰åˆä½œç”¨ä¸»è¦ç™¼ç”Ÿåœ¨è‘‰ç¶ é«”çš„é¡å›Šé«”è†œä¸Šï¼Œæ¶‰åŠå…‰åæ‡‰å’Œæš—åæ‡‰å…©å€‹éšæ®µã€‚"
        },
        {
            "type": "Human",
            "text": "æ•‘å‘½å•Šï¼æˆ‘åˆšåˆšæŠŠæ‰‹æ©Ÿå¿˜åœ¨è¨ˆç¨‹è»Šä¸Šäº†ï¼Œç¾åœ¨æ•´å€‹äººè¶…ç„¦æ…®ã€‚è£¡é¢æœ‰æˆ‘æ‰€æœ‰çš„ç…§ç‰‡é‚„æœ‰æ²’å‚™ä»½çš„è¯çµ¡äººè³‡æ–™ï¼Œå¦‚æœæ‰¾ä¸å›ä¾†æˆ‘çœŸçš„æœƒå´©æ½°ã€‚å¸æ©Ÿå¤§å“¥ä¹Ÿä¸æ¥é›»è©±ï¼Œå®¢æœåˆä¸€ç›´å¿™ç·šä¸­ï¼Œé€™åˆ°åº•æ˜¯ç”šéº¼å€’æ¥£çš„ä¸€å¤©ï¼Ÿæ‹œè¨—å¥½å¿ƒäººå¦‚æœæ’¿åˆ°å¯ä»¥é€å»è­¦å¯Ÿå±€ï¼Œæˆ‘é¡˜æ„è«‹ä½ åƒå¤§é¤ç­”è¬ï¼ŒçœŸçš„æ‹œè¨—äº†ï¼"
        }
    ]
}

# --- Session State åˆå§‹åŒ– ---
# æˆ‘å€‘éœ€è¦è¨˜ä½å…©å€‹è®Šæ•¸ï¼š
# 1. input_text: è¼¸å…¥æ¡†ç›®å‰çš„å…§å®¹
# 2. example_index: ç›®å‰è¼ªæ’­åˆ°ç¬¬å¹¾å€‹ç¯„ä¾‹
if 'input_text' not in st.session_state:
    st.session_state['input_text'] = ""
if 'example_index' not in st.session_state:
    st.session_state['example_index'] = 0

# --- å´é‚Šæ¬„è¨­å®š ---
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š (Settings)")
    # æ³¨æ„ï¼šé€™è£¡åŠ ä¸Š keyï¼Œè®“ streamlit è‡ªå‹•æ›´æ–°è®Šæ•¸
    lang_mode = st.radio(
        "é¸æ“‡èªè¨€æ¨¡å¼ (Language Mode)",
        ["Traditional Chinese (ç¹ä¸­)", "English"]
    )
    st.info("â„¹ï¸ ä¸­æ–‡æ¨¡å¼ä½¿ç”¨ `jieba` æ–·è©ï¼›è‹±æ–‡æ¨¡å¼ä½¿ç”¨ç©ºç™½åˆ‡åˆ†ã€‚")

st.title(f"ğŸ“Š {lang_mode.split('(')[0]} æ–‡æœ¬åˆ†æå™¨")

# --- åœç”¨è© ---
STOPWORDS_EN = set(['the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were', 'of', 'in', 'on', 'at', 'to', 'it', 'this', 'that'])
STOPWORDS_ZH = set(['çš„', 'äº†', 'å’Œ', 'æ˜¯', 'å°±', 'éƒ½', 'è€Œ', 'åŠ', 'èˆ‡', 'è‘—', 'æˆ–', 'ä¸€å€‹', 'æ²’æœ‰', 'æˆ‘å€‘', 'ä½ å€‘', 'ä»–å€‘', 'åœ¨', 'é€™', 'é‚£'])

# --- æ ¸å¿ƒé‚è¼¯ (ç¶­æŒä¸è®Š) ---
def analyze_text_features(text, mode):
    clean_text = text.strip()
    if not clean_text: return None

    sentences, words, stopwords = [], [], []

    if mode == "English":
        sentences = re.split(r'[.!?\n]+', clean_text)
        words = re.findall(r'\w+', clean_text.lower())
        stopwords = STOPWORDS_EN
    else:
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', clean_text)
        words = list(jieba.cut(clean_text))
        words = [w for w in words if w.strip() and len(w) > 0]
        stopwords = STOPWORDS_ZH

    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]
    if len(words) < 5: return None

    if mode == "English":
        sentence_lengths = [len(s.split()) for s in sentences]
    else:
        sentence_lengths = [len(list(jieba.cut(s))) for s in sentences]
    
    avg_len = np.mean(sentence_lengths)
    std_dev = np.std(sentence_lengths) if len(sentence_lengths) > 1 else 0

    unique_words = set(words)
    ttr = len(unique_words) / len(words)
    filtered_words = [w for w in words if w not in stopwords and len(w) > 1]
    word_counts = Counter(filtered_words)

    score = 0.5 
    if std_dev < 4: score += 0.25      # AI (å¹³ç©©)
    elif std_dev > 10: score -= 0.25   # Human (æ³¢å‹•)
    if ttr < 0.4: score += 0.15        # AI (é‡è¤‡)
    elif ttr > 0.65: score -= 0.15     # Human (è±å¯Œ)
    final_score = min(max(score, 0.01), 0.99)
    
    return {
        "score": final_score, "sentences": sentences, "sentence_lengths": sentence_lengths,
        "avg_len": avg_len, "std_dev": std_dev, "ttr": ttr, "word_counts": word_counts,
        "total_sentences": len(sentences)
    }

# --- UI ä»‹é¢ ---
col_input, col_result = st.columns([1, 2])

with col_input:
    st.subheader("ğŸ“ è¼¸å…¥å€")
    
    # --- ğŸ² ç¯„ä¾‹æŒ‰éˆ•å€å¡Š ---
    # ä½¿ç”¨ callback å‡½æ•¸ä¾†æ›´æ–° session stateï¼Œé¿å…é‚è¼¯æ··äº‚
    def load_next_example():
        # æ±ºå®šç›®å‰çš„èªè¨€ key
        dict_key = "English" if "English" in lang_mode else "Traditional Chinese (ç¹ä¸­)"
        examples = EXAMPLES[dict_key]
        
        # å–å¾—ç›®å‰çš„ index
        idx = st.session_state['example_index'] % len(examples)
        
        # æ›´æ–°è¼¸å…¥æ¡†æ–‡å­—
        selected_example = examples[idx]
        st.session_state['input_text'] = selected_example['text']
        
        # é¡¯ç¤º Toast æç¤º (çŸ­æš«å‡ºç¾çš„è¨Šæ¯)
        st.toast(f"å·²è¼‰å…¥ç¯„ä¾‹ #{idx+1}", icon="âœ…")
        
        # Index + 1 æº–å‚™ä¸‹ä¸€æ¬¡
        st.session_state['example_index'] += 1

    st.button("ğŸ² è¼‰å…¥ç¯„ä¾‹", on_click=load_next_example, type="secondary")

    # --- æ–‡å­—è¼¸å…¥æ¡† ---
    # é€™è£¡å°‡ key ç¶å®šåˆ° 'input_text'ï¼Œé€™æ¨£æŒ‰éˆ•æ›´æ–° state æ™‚ï¼Œé€™è£¡æœƒè‡ªå‹•è®Š
    user_input = st.text_area(
        "Input Text",
        height=300, 
        placeholder="è«‹è¼¸å…¥æ–‡å­—æˆ–é»æ“Šä¸Šæ–¹ç¯„ä¾‹æŒ‰éˆ•...", 
        label_visibility="collapsed",
        key="input_text" 
    )
    
    analyze_btn = st.button("ğŸš€ é–‹å§‹æ·±åº¦åˆ†æ", type="primary")

# --- åˆ†æçµæœé¡¯ç¤º (ç¶­æŒä¸è®Š) ---
if analyze_btn and user_input:
    data = analyze_text_features(user_input, lang_mode)
    
    if data is None:
        st.warning("âš ï¸ æ–‡æœ¬éçŸ­ï¼Œç„¡æ³•åˆ†æã€‚")
    else:
        with col_result:
            st.subheader("ğŸ” åˆ†æå ±å‘Š")
            
            ai_score = data['score']
            if ai_score > 0.6:
                result_text = "é«˜åº¦ç–‘ä¼¼ AI ç”Ÿæˆ"
                result_color = "red"
            elif ai_score < 0.4:
                result_text = "å¯èƒ½æ˜¯ Human æ’°å¯«"
                result_color = "green"
            else:
                result_text = "æ··åˆç‰¹å¾µ / ä¸ç¢ºå®š"
                result_color = "orange"

            st.markdown(f"""
            <div style="padding:15px; border-radius:10px; background-color:rgba(128,128,128,0.1); border-left: 5px solid {result_color}">
                <h3 style="margin:0; color:{result_color}">{result_text} (AI æŒ‡æ•¸: {int(ai_score*100)}%)</h3>
            </div>
            """, unsafe_allow_html=True)
            
            st.write("")

            kpi1, kpi2, kpi3, kpi4 = st.columns(4)
            kpi1.metric("ç¸½å¥å­æ•¸", data['total_sentences'])
            kpi2.metric("å¹³å‡å¥é•·", f"{data['avg_len']:.1f}")
            kpi3.metric("å¥é•·æ³¢å‹• (Std Dev)", f"{data['std_dev']:.1f}")
            kpi4.metric("è©å½™è±å¯Œåº¦ (TTR)", f"{data['ttr']:.2f}")

            tab1, tab2 = st.tabs(["ğŸ“ˆ å¥å‹çµæ§‹åˆ†æ", "ğŸ”  å¸¸ç”¨è©å½™çµ±è¨ˆ"])

            with tab1:
                st.caption("Human é€šå¸¸å¥é•·æ³¢å‹•å¤§ (ç·šæ¢åŠ‡çƒˆè·³å‹•)ï¼›AI å‰‡è¼ƒå¹³ç©©ã€‚")
                chart_data = pd.DataFrame({
                    "å¥å­é †åº": range(1, len(data['sentence_lengths']) + 1),
                    "è©æ•¸": data['sentence_lengths']
                })
                st.line_chart(chart_data, x="å¥å­é †åº", y="è©æ•¸", color="#FF4B4B")

            with tab2:
                top_words = data['word_counts'].most_common(10)
                if top_words:
                    words_df = pd.DataFrame(top_words, columns=["è©å½™", "æ¬¡æ•¸"])
                    st.bar_chart(words_df.set_index("è©å½™"))
                else:
                    st.info("é—œéµå­—æ•¸æ“šä¸è¶³")

elif not analyze_btn:
    with col_result:
        st.info("ğŸ‘ˆ é»æ“Šã€ŒğŸ² è¼‰å…¥ç¯„ä¾‹ã€å¿«é€Ÿé«”é©—åŠŸèƒ½ï¼Œæˆ–è‡ªè¡Œè¼¸å…¥æ–‡ç« ã€‚")